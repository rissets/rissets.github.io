{"componentChunkName":"component---src-templates-post-js","path":"/pensieve/rag-with-django-langchain/","result":{"data":{"markdownRemark":{"html":"<h2>Introduction</h2>\n<p>Retrieval-Augmented Generation (RAG) has become the standard pattern for building LLM applications that need access to private or domain-specific knowledge. Instead of fine-tuning a model on your data (expensive, slow, inflexible), RAG retrieves relevant context at query time and feeds it to the LLM alongside the user's question.</p>\n<p>In this post, I'll walk through how to build a production-ready RAG system using <strong>Django</strong> as the backend framework and <strong>Langchain</strong> for LLM orchestration — a stack I've used extensively in building AI-powered applications.</p>\n<h2>Architecture Overview</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">User Query → Django API → Langchain Pipeline → Response\n                              │\n                    ┌─────────┼──────────┐\n                    ▼         ▼          ▼\n              Embedding    Vector DB   LLM API\n              Model        (pgvector)  (Ollama/OpenAI)</code></pre></div>\n<p>The system consists of:</p>\n<ol>\n<li><strong>Django REST API</strong> — handles authentication, rate limiting, and request routing</li>\n<li><strong>Document ingestion pipeline</strong> — processes PDFs/text, chunks them, and stores embeddings</li>\n<li><strong>Retrieval engine</strong> — performs semantic search using pgvector</li>\n<li><strong>Generation layer</strong> — Langchain chains that combine retrieved context with LLM prompting</li>\n</ol>\n<h2>Setting Up the Stack</h2>\n<h3>Django Models</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># models.py</span>\n<span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>db <span class=\"token keyword\">import</span> models\n<span class=\"token keyword\">from</span> pgvector<span class=\"token punctuation\">.</span>django <span class=\"token keyword\">import</span> VectorField\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Document</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    title <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>CharField<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">500</span><span class=\"token punctuation\">)</span>\n    source <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>FileField<span class=\"token punctuation\">(</span>upload_to<span class=\"token operator\">=</span><span class=\"token string\">'documents/'</span><span class=\"token punctuation\">)</span>\n    uploaded_at <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>DateTimeField<span class=\"token punctuation\">(</span>auto_now_add<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DocumentChunk</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    document <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>ForeignKey<span class=\"token punctuation\">(</span>Document<span class=\"token punctuation\">,</span> on_delete<span class=\"token operator\">=</span>models<span class=\"token punctuation\">.</span>CASCADE<span class=\"token punctuation\">)</span>\n    content <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>TextField<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    embedding <span class=\"token operator\">=</span> VectorField<span class=\"token punctuation\">(</span>dimensions<span class=\"token operator\">=</span><span class=\"token number\">1536</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># OpenAI ada-002</span>\n    metadata <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>JSONField<span class=\"token punctuation\">(</span>default<span class=\"token operator\">=</span><span class=\"token builtin\">dict</span><span class=\"token punctuation\">)</span>\n    chunk_index <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>IntegerField<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">class</span> <span class=\"token class-name\">Meta</span><span class=\"token punctuation\">:</span>\n        indexes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n            models<span class=\"token punctuation\">.</span>Index<span class=\"token punctuation\">(</span>fields<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">]</span></code></pre></div>\n<h3>Document Ingestion</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/ingestion.py</span>\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>text_splitter <span class=\"token keyword\">import</span> RecursiveCharacterTextSplitter\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>embeddings <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">ingest_document</span><span class=\"token punctuation\">(</span>document_id<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    document <span class=\"token operator\">=</span> Document<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span>document_id<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Load and split</span>\n    text <span class=\"token operator\">=</span> extract_text<span class=\"token punctuation\">(</span>document<span class=\"token punctuation\">.</span>source<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span>\n    splitter <span class=\"token operator\">=</span> RecursiveCharacterTextSplitter<span class=\"token punctuation\">(</span>\n        chunk_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span>\n        chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span>\n        separators<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"\\n\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\". \"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n    chunks <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Embed</span>\n    embeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    vectors <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">.</span>embed_documents<span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Store</span>\n    DocumentChunk<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>bulk_create<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n        DocumentChunk<span class=\"token punctuation\">(</span>\n            document<span class=\"token operator\">=</span>document<span class=\"token punctuation\">,</span>\n            content<span class=\"token operator\">=</span>chunk<span class=\"token punctuation\">,</span>\n            embedding<span class=\"token operator\">=</span>vector<span class=\"token punctuation\">,</span>\n            chunk_index<span class=\"token operator\">=</span>i\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>chunk<span class=\"token punctuation\">,</span> vector<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">,</span> vectors<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Retrieval with pgvector</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/retrieval.py</span>\n<span class=\"token keyword\">from</span> pgvector<span class=\"token punctuation\">.</span>django <span class=\"token keyword\">import</span> L2Distance\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>embeddings <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">retrieve_context</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> top_k<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span> <span class=\"token operator\">=</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    embeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    query_vector <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">.</span>embed_query<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n\n    chunks <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>\n        DocumentChunk<span class=\"token punctuation\">.</span>objects\n        <span class=\"token punctuation\">.</span>annotate<span class=\"token punctuation\">(</span>distance<span class=\"token operator\">=</span>L2Distance<span class=\"token punctuation\">(</span><span class=\"token string\">'embedding'</span><span class=\"token punctuation\">,</span> query_vector<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">.</span>order_by<span class=\"token punctuation\">(</span><span class=\"token string\">'distance'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>top_k<span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> chunk<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">:</span> chunk<span class=\"token punctuation\">.</span>distance<span class=\"token punctuation\">}</span>\n        <span class=\"token keyword\">for</span> chunk <span class=\"token keyword\">in</span> chunks\n    <span class=\"token punctuation\">]</span></code></pre></div>\n<h3>The RAG Chain</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/rag.py</span>\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>chains <span class=\"token keyword\">import</span> RetrievalQA\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>prompts <span class=\"token keyword\">import</span> PromptTemplate\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>llms <span class=\"token keyword\">import</span> Ollama\n\nPROMPT_TEMPLATE <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"Use the following context to answer the question.\nIf you don't know the answer, say so — don't make things up.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">generate_answer</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    context_docs <span class=\"token operator\">=</span> retrieve_context<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    context <span class=\"token operator\">=</span> <span class=\"token string\">\"\\n\\n\"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>doc<span class=\"token punctuation\">[</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> context_docs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    llm <span class=\"token operator\">=</span> Ollama<span class=\"token punctuation\">(</span>model<span class=\"token operator\">=</span><span class=\"token string\">\"llama3\"</span><span class=\"token punctuation\">)</span>\n    prompt <span class=\"token operator\">=</span> PromptTemplate<span class=\"token punctuation\">(</span>\n        template<span class=\"token operator\">=</span>PROMPT_TEMPLATE<span class=\"token punctuation\">,</span>\n        input_variables<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"context\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"question\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n\n    chain <span class=\"token operator\">=</span> prompt <span class=\"token operator\">|</span> llm\n    response <span class=\"token operator\">=</span> chain<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"context\"</span><span class=\"token punctuation\">:</span> context<span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"question\"</span><span class=\"token punctuation\">:</span> query\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"answer\"</span><span class=\"token punctuation\">:</span> response<span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"sources\"</span><span class=\"token punctuation\">:</span> context_docs\n    <span class=\"token punctuation\">}</span></code></pre></div>\n<h2>Django API Endpoint</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># views.py</span>\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>views <span class=\"token keyword\">import</span> APIView\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>response <span class=\"token keyword\">import</span> Response\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>permissions <span class=\"token keyword\">import</span> IsAuthenticated\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">RAGQueryView</span><span class=\"token punctuation\">(</span>APIView<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    permission_classes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>IsAuthenticated<span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> request<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        query <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token string\">\"query\"</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> query<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"error\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Query is required\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> status<span class=\"token operator\">=</span><span class=\"token number\">400</span><span class=\"token punctuation\">)</span>\n\n        result <span class=\"token operator\">=</span> generate_answer<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Production Considerations</h2>\n<h3>1. Chunking Strategy Matters</h3>\n<p>The quality of your RAG system depends heavily on how you split documents. We found that:</p>\n<ul>\n<li><strong>1000 tokens per chunk</strong> with <strong>200 token overlap</strong> works well for most documents</li>\n<li>Using semantic boundaries (paragraphs, sections) preserves context better than fixed-size splits</li>\n<li>Metadata-enriched chunks (source, page number, section title) improve retrieval quality</li>\n</ul>\n<h3>2. Hybrid Search</h3>\n<p>Pure vector similarity search can miss keyword-specific matches. We combine:</p>\n<ul>\n<li><strong>Semantic search</strong> (pgvector) for conceptual similarity</li>\n<li><strong>Full-text search</strong> (PostgreSQL <code class=\"language-text\">tsvector</code>) for keyword matching</li>\n<li><strong>Reciprocal Rank Fusion</strong> to combine results</li>\n</ul>\n<h3>3. Caching</h3>\n<p>For repeated queries, we cache embeddings and responses using Redis with a TTL:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> hashlib\n<span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>core<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> cache\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cached_generate</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    cache_key <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"rag:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>hashlib<span class=\"token punctuation\">.</span>md5<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>hexdigest<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span>\n    cached <span class=\"token operator\">=</span> cache<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>cache_key<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> cached<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> cached\n\n    result <span class=\"token operator\">=</span> generate_answer<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    cache<span class=\"token punctuation\">.</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>cache_key<span class=\"token punctuation\">,</span> result<span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span><span class=\"token number\">3600</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> result</code></pre></div>\n<h3>4. Evaluation</h3>\n<p>We use RAGAS metrics to evaluate retrieval quality: faithfulness, answer relevancy, and context precision. This helps us tune chunk size, overlap, and retrieval parameters.</p>\n<h2>Conclusion</h2>\n<p>Django + Langchain is a powerful combination for building production RAG systems. Django handles the \"boring but essential\" parts (auth, admin, ORM, migrations) while Langchain provides the LLM orchestration layer. With pgvector, you get a vector database without adding another service to your stack.</p>\n<hr>\n<p><em>This post is based on my experience building AI systems at Orbit Tech Solution and Bagus Harapan Tritunggal.</em></p>","frontmatter":{"title":"Building a RAG System with Django and Langchain","description":"How to build a production-ready Retrieval-Augmented Generation system using Django as the backend and Langchain for orchestration.","date":"2025-03-20","slug":"/pensieve/rag-with-django-langchain","tags":["AI","Django","Langchain","LLM"]}}},"pageContext":{"slug":"/pensieve/rag-with-django-langchain"}},"staticQueryHashes":["3115057458"],"slicesMap":{}}
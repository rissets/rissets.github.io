{"componentChunkName":"component---src-pages-pensieve-index-js","path":"/pensieve/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"title":"Building Enterprise Computer Vision Systems: From Research to Production","description":"Key architectural decisions and lessons learned from deploying face recognition and fish classification systems at scale in production environments.","slug":"/pensieve/computer-vision-production","date":"2025-12-10","tags":["Computer Vision","Python","Django","Machine Learning","PyTorch"],"draft":false},"html":"<h2>Introduction</h2>\n<p>Deploying computer vision systems into production is a fundamentally different challenge from training a model that performs well on a benchmark. At Pt Bagus Harapan Tritunggal, I had the opportunity to build two distinct production CV systems: an <strong>Enterprise Face Recognition API</strong> for biometric verification and a <strong>Fish Recognition System</strong> for KNMP (National Fisheries Management Council). This post captures the key lessons and architectural decisions from both projects.</p>\n<h2>The Gap Between Research and Production</h2>\n<p>A model with 99% accuracy on your test set can fail spectacularly in production due to:</p>\n<ul>\n<li><strong>Distribution shift</strong> — real-world images differ from training data (lighting, angle, occlusion)</li>\n<li><strong>Adversarial inputs</strong> — deliberate attempts to fool the system (especially critical for biometrics)</li>\n<li><strong>Latency constraints</strong> — users expect responses in milliseconds, not seconds</li>\n<li><strong>Throughput requirements</strong> — handling concurrent requests at scale</li>\n</ul>\n<h3>Face Recognition: Security-Critical Deployment</h3>\n<p>The face recognition system required not just high accuracy, but <strong>anti-spoofing</strong> capabilities — the ability to distinguish a live face from a photograph or video replay attack. This added an entire layer of complexity to the pipeline.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">FaceVerificationPipeline</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Detection: Ultralytics YOLO for face detection</span>\n        self<span class=\"token punctuation\">.</span>detector <span class=\"token operator\">=</span> YOLO<span class=\"token punctuation\">(</span><span class=\"token string\">'face_detection.pt'</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Recognition: InsightFace for face embeddings</span>\n        self<span class=\"token punctuation\">.</span>recognizer <span class=\"token operator\">=</span> FaceAnalysis<span class=\"token punctuation\">(</span>providers<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'CUDAExecutionProvider'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>recognizer<span class=\"token punctuation\">.</span>prepare<span class=\"token punctuation\">(</span>ctx_id<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> det_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">640</span><span class=\"token punctuation\">,</span> <span class=\"token number\">640</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Liveness: MediaPipe for landmark detection + custom anti-spoof model</span>\n        self<span class=\"token punctuation\">.</span>liveness_checker <span class=\"token operator\">=</span> LivenessDetector<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">verify</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> image<span class=\"token punctuation\">:</span> np<span class=\"token punctuation\">.</span>ndarray<span class=\"token punctuation\">,</span> enrolled_embedding<span class=\"token punctuation\">:</span> np<span class=\"token punctuation\">.</span>ndarray<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Step 1: Detect faces</span>\n        faces <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>detector<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>faces<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"status\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"no_face_detected\"</span><span class=\"token punctuation\">}</span>\n\n        <span class=\"token comment\"># Step 2: Liveness check — reject spoofing attempts</span>\n        liveness_score <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>liveness_checker<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> faces<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> liveness_score <span class=\"token operator\">&lt;</span> LIVENESS_THRESHOLD<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"status\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"spoof_detected\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"confidence\"</span><span class=\"token punctuation\">:</span> liveness_score<span class=\"token punctuation\">}</span>\n\n        <span class=\"token comment\"># Step 3: Extract embedding and compare</span>\n        face_embedding <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>recognizer<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>embedding\n        similarity <span class=\"token operator\">=</span> cosine_similarity<span class=\"token punctuation\">(</span>face_embedding<span class=\"token punctuation\">,</span> enrolled_embedding<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token string\">\"status\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"verified\"</span> <span class=\"token keyword\">if</span> similarity <span class=\"token operator\">></span> SIMILARITY_THRESHOLD <span class=\"token keyword\">else</span> <span class=\"token string\">\"not_matched\"</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">\"similarity\"</span><span class=\"token punctuation\">:</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span>similarity<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">\"liveness_score\"</span><span class=\"token punctuation\">:</span> <span class=\"token builtin\">float</span><span class=\"token punctuation\">(</span>liveness_score<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">}</span></code></pre></div>\n<h3>Fish Recognition: Domain-Specific Data Challenges</h3>\n<p>The fish classification system presented different challenges. The primary hurdle was <strong>data scarcity</strong> — getting labeled images of specific fish species in Indonesian waters at sufficient volume and quality.</p>\n<p>Our approach:</p>\n<ol>\n<li><strong>Data collection</strong> — partnered with KNMP to gather images from field surveys</li>\n<li><strong>Data augmentation</strong> — extensive augmentation pipeline (rotation, flipping, color jitter, Cutout) to artificially expand the dataset</li>\n<li><strong>Transfer learning</strong> — fine-tuned a pre-trained EfficientNet-B4 on our domain-specific dataset</li>\n<li><strong>Active learning</strong> — deployed a confidence-threshold-based system to flag uncertain predictions for human review</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Training with class imbalance handling</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">FishClassificationTrainer</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> num_classes<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>model <span class=\"token operator\">=</span> timm<span class=\"token punctuation\">.</span>create_model<span class=\"token punctuation\">(</span>\n            <span class=\"token string\">'efficientnet_b4'</span><span class=\"token punctuation\">,</span>\n            pretrained<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n            num_classes<span class=\"token operator\">=</span>num_classes\n        <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">compute_class_weights</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> dataset<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"Handle class imbalance with weighted sampling.\"\"\"</span>\n        class_counts <span class=\"token operator\">=</span> Counter<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">.</span>labels<span class=\"token punctuation\">)</span>\n        total <span class=\"token operator\">=</span> <span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>class_counts<span class=\"token punctuation\">.</span>values<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        weights <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>total <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>class_counts<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> class_counts<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n                   <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>class_counts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>FloatTensor<span class=\"token punctuation\">(</span>weights<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> train_loader<span class=\"token punctuation\">,</span> val_loader<span class=\"token punctuation\">,</span> epochs<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        class_weights <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>compute_class_weights<span class=\"token punctuation\">(</span>train_loader<span class=\"token punctuation\">.</span>dataset<span class=\"token punctuation\">)</span>\n        criterion <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span>weight<span class=\"token operator\">=</span>class_weights<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        optimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>AdamW<span class=\"token punctuation\">(</span>\n            self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">1e-4</span><span class=\"token punctuation\">,</span> weight_decay<span class=\"token operator\">=</span><span class=\"token number\">0.01</span>\n        <span class=\"token punctuation\">)</span>\n        scheduler <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>lr_scheduler<span class=\"token punctuation\">.</span>CosineAnnealingLR<span class=\"token punctuation\">(</span>\n            optimizer<span class=\"token punctuation\">,</span> T_max<span class=\"token operator\">=</span>epochs\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># ... training loop</span></code></pre></div>\n<h2>Serving Architecture</h2>\n<p>Both systems needed to be served as REST APIs with low latency. The architecture we landed on:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">                          ┌─────────────────────────┐\n                          │     Load Balancer         │\n                          └──────────┬──────────────┘\n                                     │\n              ┌──────────────────────┼──────────────────────┐\n              ▼                      ▼                       ▼\n    ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐\n    │   Django API     │  │   Django API     │  │   Django API     │\n    │  (Worker 1)      │  │  (Worker 2)      │  │  (Worker 3)      │\n    └────────┬─────────┘  └────────┬─────────┘  └────────┬─────────┘\n             │                     │                      │\n             └─────────────────────┼──────────────────────┘\n                                   │\n             ┌─────────────────────┼──────────────────────┐\n             ▼                     ▼                       ▼\n    ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐\n    │  Model Server    │  │   PostgreSQL     │  │     Redis        │\n    │  (TorchServe)    │  │   (Embeddings)   │  │    (Cache)       │\n    └──────────────────┘  └──────────────────┘  └──────────────────┘</code></pre></div>\n<p><strong>Key decisions:</strong></p>\n<ul>\n<li><strong>TorchServe</strong> for dedicated model inference — separates the ML runtime from the Django app, enabling independent scaling</li>\n<li><strong>Redis</strong> for caching face embeddings — embedding lookup on every request would be prohibitively slow</li>\n<li><strong>PostgreSQL with pgvector</strong> for scalable similarity search across enrolled face embeddings</li>\n</ul>\n<h2>Performance Optimizations</h2>\n<h3>Model Quantization</h3>\n<p>For the face recognition model, INT8 quantization reduced model size by ~4x and inference time by ~2x with minimal accuracy degradation:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>quantization\n\n<span class=\"token comment\"># Post-training static quantization</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>qconfig <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>quantization<span class=\"token punctuation\">.</span>get_default_qconfig<span class=\"token punctuation\">(</span><span class=\"token string\">'fbgemm'</span><span class=\"token punctuation\">)</span>\ntorch<span class=\"token punctuation\">.</span>quantization<span class=\"token punctuation\">.</span>prepare<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Calibration</span>\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> images<span class=\"token punctuation\">,</span> _ <span class=\"token keyword\">in</span> calibration_loader<span class=\"token punctuation\">:</span>\n        model<span class=\"token punctuation\">(</span>images<span class=\"token punctuation\">)</span>\n\ntorch<span class=\"token punctuation\">.</span>quantization<span class=\"token punctuation\">.</span>convert<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Batch Processing</h3>\n<p>For the fish recognition API, requests are batched using an async queue to amortize GPU overhead:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">BatchInferenceQueue</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> max_batch_size<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span> max_wait_ms<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>queue <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>Queue<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>max_batch_size <span class=\"token operator\">=</span> max_batch_size\n        self<span class=\"token punctuation\">.</span>max_wait_ms <span class=\"token operator\">=</span> max_wait_ms\n\n    <span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">predict</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> image<span class=\"token punctuation\">:</span> np<span class=\"token punctuation\">.</span>ndarray<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">:</span>\n        future <span class=\"token operator\">=</span> asyncio<span class=\"token punctuation\">.</span>Future<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">await</span> self<span class=\"token punctuation\">.</span>queue<span class=\"token punctuation\">.</span>put<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>image<span class=\"token punctuation\">,</span> future<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> <span class=\"token keyword\">await</span> future\n\n    <span class=\"token keyword\">async</span> <span class=\"token keyword\">def</span> <span class=\"token function\">process_batches</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">while</span> <span class=\"token boolean\">True</span><span class=\"token punctuation\">:</span>\n            batch <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n            deadline <span class=\"token operator\">=</span> time<span class=\"token punctuation\">.</span>monotonic<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>max_wait_ms <span class=\"token operator\">/</span> <span class=\"token number\">1000</span>\n\n            <span class=\"token keyword\">while</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>batch<span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> self<span class=\"token punctuation\">.</span>max_batch_size<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                    timeout <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> deadline <span class=\"token operator\">-</span> time<span class=\"token punctuation\">.</span>monotonic<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                    item <span class=\"token operator\">=</span> <span class=\"token keyword\">await</span> asyncio<span class=\"token punctuation\">.</span>wait_for<span class=\"token punctuation\">(</span>\n                        self<span class=\"token punctuation\">.</span>queue<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span>timeout\n                    <span class=\"token punctuation\">)</span>\n                    batch<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">except</span> asyncio<span class=\"token punctuation\">.</span>TimeoutError<span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">break</span>\n\n            <span class=\"token keyword\">if</span> batch<span class=\"token punctuation\">:</span>\n                images<span class=\"token punctuation\">,</span> futures <span class=\"token operator\">=</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>batch<span class=\"token punctuation\">)</span>\n                results <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict_batch<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span>images<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">for</span> future<span class=\"token punctuation\">,</span> result <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>futures<span class=\"token punctuation\">,</span> results<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    future<span class=\"token punctuation\">.</span>set_result<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Monitoring in Production</h2>\n<p>Both systems are monitored via:</p>\n<ul>\n<li><strong>Prometheus + Grafana</strong> for latency percentiles (p50, p95, p99) and throughput</li>\n<li><strong>Confidence score distribution tracking</strong> — a drop in average confidence often signals data drift</li>\n<li><strong>Rejection rate monitoring</strong> — for the face recognition system, a spike in anti-spoof rejections can indicate an attack attempt</li>\n</ul>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Liveness detection is non-negotiable for biometrics</strong> — without it, a printed photo can fool even a highly accurate recognition system</li>\n<li><strong>Data quality beats model complexity</strong> — 1000 high-quality, diverse training images outperform 10,000 noisy ones</li>\n<li><strong>Separate your ML runtime from your API</strong> — Django is excellent for the business logic; TorchServe handles the model serving</li>\n<li><strong>Cache aggressively</strong> — face embeddings, model outputs for known images, and preprocessing results can all be cached</li>\n<li><strong>Build with confidence thresholds, not binary outputs</strong> — returning a confidence score enables the application layer to handle edge cases gracefully</li>\n</ol>\n<hr>\n<p><em>This post is based on my work at Pt Bagus Harapan Tritunggal building production computer vision systems.</em></p>"}},{"node":{"frontmatter":{"title":"Building Agentic AI Systems: From LLM Chains to Autonomous Policy Simulators","description":"How we built an Agentic AI system for the Indonesian Ministry of Law to simulate and analyze the impact of new legal policies before enactment.","slug":"/pensieve/building-agentic-ai-systems","date":"2025-09-15","tags":["AI","Agentic AI","Langchain","LLM","Python"],"draft":false},"html":"<h2>Introduction</h2>\n<p>Agentic AI — systems where LLMs act as autonomous reasoning engines that plan, use tools, and execute multi-step workflows — is rapidly moving from research curiosity to production reality. I had the unique opportunity to build an <strong>AI Policy Simulator</strong> for Indonesia's Ministry of Law and Human Rights (Kemenkumham), a system that enables government officials to model the potential real-world impacts of new legal policies before they're officially enacted.</p>\n<p>This post covers the architecture, key design decisions, and lessons learned from building a production agentic AI system for a high-stakes government use case.</p>\n<h2>The Problem: Policy Impact Simulation</h2>\n<p>Before a new regulation is enacted within the AHU Online ecosystem (Indonesia's legal entity management system), decision-makers need answers to complex questions:</p>\n<ul>\n<li>\"If we introduce this new compliance requirement, how many businesses will be affected?\"</li>\n<li>\"What's the estimated processing time impact on current AHU workflows?\"</li>\n<li>\"Are there conflicting regulations this new policy would interact with?\"</li>\n</ul>\n<p>These questions require reasoning over large document corpora, structured databases, and historical workflow data — exactly the kind of multi-step, tool-augmented reasoning that agentic AI excels at.</p>\n<h2>Architecture: The Multi-Agent Approach</h2>\n<p>Rather than a single monolithic LLM chain, we used a <strong>multi-agent architecture</strong> where specialized agents handle different aspects of the simulation:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">User Query\n    │\n    ▼\n┌─────────────────────────────────────────────────────┐\n│                  Orchestrator Agent                   │\n│  (Plans the simulation, delegates to sub-agents)     │\n└──────────┬──────────────────────────────────────────┘\n           │\n    ┌──────┴──────┐\n    │             │\n    ▼             ▼\n┌────────┐   ┌────────────┐\n│ Legal  │   │ Workflow   │\n│ Analyst│   │ Analyst    │\n│ Agent  │   │ Agent      │\n└────┬───┘   └─────┬──────┘\n     │              │\n     ▼              ▼\n Document      Database\n Retrieval     Queries\n (RAG)         (SQL)</code></pre></div>\n<h3>The Orchestrator</h3>\n<p>The orchestrator uses an LLM (Llama 3 via Ollama) to break down the user's policy question into a simulation plan:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>agents <span class=\"token keyword\">import</span> AgentExecutor<span class=\"token punctuation\">,</span> create_openai_tools_agent\n<span class=\"token keyword\">from</span> langchain_core<span class=\"token punctuation\">.</span>prompts <span class=\"token keyword\">import</span> ChatPromptTemplate<span class=\"token punctuation\">,</span> MessagesPlaceholder\n\nORCHESTRATOR_SYSTEM <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"You are a policy simulation orchestrator for the Indonesian\nMinistry of Law. Given a proposed policy, create a structured simulation plan by:\n1. Identifying what aspects of the AHU system would be affected\n2. Estimating the scope of impact (number of affected entities)\n3. Analyzing potential conflicts with existing regulations\n4. Projecting workflow changes and processing time impacts\n\nUse the available tools to gather data and reason systematically.\"\"\"</span>\n\nprompt <span class=\"token operator\">=</span> ChatPromptTemplate<span class=\"token punctuation\">.</span>from_messages<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">(</span><span class=\"token string\">\"system\"</span><span class=\"token punctuation\">,</span> ORCHESTRATOR_SYSTEM<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    MessagesPlaceholder<span class=\"token punctuation\">(</span>variable_name<span class=\"token operator\">=</span><span class=\"token string\">\"chat_history\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">(</span><span class=\"token string\">\"human\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"{input}\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    MessagesPlaceholder<span class=\"token punctuation\">(</span>variable_name<span class=\"token operator\">=</span><span class=\"token string\">\"agent_scratchpad\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\norchestrator <span class=\"token operator\">=</span> create_openai_tools_agent<span class=\"token punctuation\">(</span>llm<span class=\"token punctuation\">,</span> tools<span class=\"token punctuation\">,</span> prompt<span class=\"token punctuation\">)</span>\norchestrator_executor <span class=\"token operator\">=</span> AgentExecutor<span class=\"token punctuation\">(</span>\n    agent<span class=\"token operator\">=</span>orchestrator<span class=\"token punctuation\">,</span>\n    tools<span class=\"token operator\">=</span>tools<span class=\"token punctuation\">,</span>\n    verbose<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    max_iterations<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span>\n    handle_parsing_errors<span class=\"token operator\">=</span><span class=\"token boolean\">True</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Tools: The Bridge to Real Data</h3>\n<p>The power of the agentic approach comes from tools — functions the LLM can call to interact with real systems:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>tools <span class=\"token keyword\">import</span> tool\n<span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>db <span class=\"token keyword\">import</span> connection\n\n<span class=\"token decorator annotation punctuation\">@tool</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">query_affected_entities</span><span class=\"token punctuation\">(</span>policy_type<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> criteria<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Query the AHU database to count entities affected by a policy change.\n\n    Args:\n        policy_type: Type of legal entity affected (PT, CV, Yayasan, etc.)\n        criteria: SQL-safe criteria string for filtering entities\n\n    Returns:\n        JSON string with count and representative sample of affected entities\n    \"\"\"</span>\n    <span class=\"token keyword\">with</span> connection<span class=\"token punctuation\">.</span>cursor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> cursor<span class=\"token punctuation\">:</span>\n        cursor<span class=\"token punctuation\">.</span>execute<span class=\"token punctuation\">(</span><span class=\"token triple-quoted-string string\">\"\"\"\n            SELECT entity_type, COUNT(*) as count,\n                   AVG(processing_days) as avg_processing\n            FROM ahu_entities\n            WHERE entity_type = %s AND %s\n            GROUP BY entity_type\n        \"\"\"</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>policy_type<span class=\"token punctuation\">,</span> criteria<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        results <span class=\"token operator\">=</span> cursor<span class=\"token punctuation\">.</span>fetchall<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"affected_count\"</span><span class=\"token punctuation\">:</span> results<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"avg_days\"</span><span class=\"token punctuation\">:</span> results<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@tool</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">search_regulations</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Semantic search over the regulation document corpus.\n\n    Args:\n        query: Natural language query about regulations\n\n    Returns:\n        Top 3 most relevant regulation excerpts with source citations\n    \"\"\"</span>\n    results <span class=\"token operator\">=</span> regulation_rag<span class=\"token punctuation\">.</span>retrieve<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">,</span> top_k<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> r<span class=\"token punctuation\">[</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"source\"</span><span class=\"token punctuation\">:</span> r<span class=\"token punctuation\">[</span><span class=\"token string\">\"metadata\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"doc_id\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span>\n        <span class=\"token keyword\">for</span> r <span class=\"token keyword\">in</span> results\n    <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@tool</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">simulate_workflow_impact</span><span class=\"token punctuation\">(</span>change_description<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Run a workflow simulation to estimate processing time impact.\n\n    Args:\n        change_description: Description of the workflow change\n\n    Returns:\n        Estimated delta in processing time and bottleneck analysis\n    \"\"\"</span>\n    <span class=\"token keyword\">return</span> workflow_simulator<span class=\"token punctuation\">.</span>estimate_impact<span class=\"token punctuation\">(</span>change_description<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Handling the Challenges of Agentic AI in Production</h2>\n<h3>1. Grounding and Hallucination Prevention</h3>\n<p>Government systems have zero tolerance for hallucinated data. We implemented multiple guardrails:</p>\n<ul>\n<li><strong>Mandatory tool use for data claims</strong>: The system prompt explicitly instructs the agent to use tools for any numerical claim, never to estimate from memory</li>\n<li><strong>Citation tracking</strong>: Every factual claim in the output is traced back to a tool call result</li>\n<li><strong>Output validation</strong>: A separate \"critic\" LLM call validates the final output against the tool call history</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">validate_simulation_output</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> tool_calls<span class=\"token punctuation\">:</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Verify all claims in the output are grounded in tool call results.\"\"\"</span>\n    critic_prompt <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"\"\"Given this simulation output and the supporting data from tool calls,\nidentify any claims that are NOT supported by the tool call results.\n\nOutput: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>output<span class=\"token punctuation\">}</span></span><span class=\"token string\">\nTool Results: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span>tool_calls<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\n\nList any unsupported claims or respond with \"VALIDATED\" if all claims are grounded.\"\"\"</span></span>\n\n    validation <span class=\"token operator\">=</span> critic_llm<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span>critic_prompt<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"valid\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"VALIDATED\"</span> <span class=\"token keyword\">in</span> validation<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">,</span> <span class=\"token string\">\"issues\"</span><span class=\"token punctuation\">:</span> validation<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">}</span></code></pre></div>\n<h3>2. Cost and Latency Management</h3>\n<p>Agentic AI systems can make many LLM calls per request. We managed this with:</p>\n<ul>\n<li><strong>Tool call caching</strong>: Results of expensive DB queries are cached for the session duration</li>\n<li><strong>Early termination</strong>: If the agent reaches a confident conclusion early, it stops before exhausting its iteration budget</li>\n<li><strong>Streaming</strong>: We stream the agent's \"thinking\" to the frontend so users see progress during long simulations</li>\n</ul>\n<h3>3. MCP (Model Context Protocol) Integration</h3>\n<p>For structured data exchange between the Django backend and the Ollama model server, we used MCP (Anthropic's Model Context Protocol). This standardizes how the application exposes tools and resources to the model:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># MCP server exposing AHU data resources</span>\n<span class=\"token keyword\">from</span> mcp<span class=\"token punctuation\">.</span>server<span class=\"token punctuation\">.</span>fastmcp <span class=\"token keyword\">import</span> FastMCP\n\nmcp <span class=\"token operator\">=</span> FastMCP<span class=\"token punctuation\">(</span><span class=\"token string\">\"AHU Policy Simulator\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@mcp<span class=\"token punctuation\">.</span>resource</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"ahu://entities/{entity_type}\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">get_entity_stats</span><span class=\"token punctuation\">(</span>entity_type<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Get statistics for a specific AHU entity type.\"\"\"</span>\n    stats <span class=\"token operator\">=</span> Entity<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span><span class=\"token builtin\">filter</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">type</span><span class=\"token operator\">=</span>entity_type<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>aggregate<span class=\"token punctuation\">(</span>\n        count<span class=\"token operator\">=</span>Count<span class=\"token punctuation\">(</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        avg_processing_days<span class=\"token operator\">=</span>Avg<span class=\"token punctuation\">(</span><span class=\"token string\">'processing_days'</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> json<span class=\"token punctuation\">.</span>dumps<span class=\"token punctuation\">(</span>stats<span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@mcp<span class=\"token punctuation\">.</span>tool</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">run_compliance_check</span><span class=\"token punctuation\">(</span>policy_text<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Check a proposed policy against existing regulations for conflicts.\"\"\"</span>\n    conflicts <span class=\"token operator\">=</span> find_regulatory_conflicts<span class=\"token punctuation\">(</span>policy_text<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span><span class=\"token string\">\"conflicts\"</span><span class=\"token punctuation\">:</span> conflicts<span class=\"token punctuation\">,</span> <span class=\"token string\">\"severity\"</span><span class=\"token punctuation\">:</span> assess_severity<span class=\"token punctuation\">(</span>conflicts<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></code></pre></div>\n<h2>Key Lessons for Production Agentic AI</h2>\n<ol>\n<li><strong>Start with a bounded task</strong> — don't give the agent unlimited scope; define clear entry/exit conditions</li>\n<li><strong>Tool design is as important as model selection</strong> — well-designed tools with clear docstrings dramatically improve reliability</li>\n<li><strong>Always validate before presenting to users</strong> — especially in high-stakes domains like government policy</li>\n<li><strong>Log everything</strong> — every tool call, every LLM response; you need this for debugging and audit trails</li>\n<li><strong>Hybrid approaches work best</strong> — combine agentic reasoning for complex queries with rule-based systems for simple lookups</li>\n</ol>\n<h2>What's Next</h2>\n<p>We're exploring <strong>multi-modal policy simulation</strong> — incorporating document images, forms, and diagrams into the context alongside text. We're also building a <strong>feedback loop</strong> where government officials can correct simulation outputs, creating a fine-tuning dataset for domain adaptation.</p>\n<hr>\n<p><em>This post is based on my work at Pt Bagus Harapan Tritunggal building the AI Policy Simulator for Kemenkumham.</em></p>"}},{"node":{"frontmatter":{"title":"Building a RAG System with Django and Langchain","description":"How to build a production-ready Retrieval-Augmented Generation system using Django as the backend and Langchain for orchestration.","slug":"/pensieve/rag-with-django-langchain","date":"2025-03-20","tags":["AI","Django","Langchain","LLM"],"draft":false},"html":"<h2>Introduction</h2>\n<p>Retrieval-Augmented Generation (RAG) has become the standard pattern for building LLM applications that need access to private or domain-specific knowledge. Instead of fine-tuning a model on your data (expensive, slow, inflexible), RAG retrieves relevant context at query time and feeds it to the LLM alongside the user's question.</p>\n<p>In this post, I'll walk through how to build a production-ready RAG system using <strong>Django</strong> as the backend framework and <strong>Langchain</strong> for LLM orchestration — a stack I've used extensively in building AI-powered applications.</p>\n<h2>Architecture Overview</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">User Query → Django API → Langchain Pipeline → Response\n                              │\n                    ┌─────────┼──────────┐\n                    ▼         ▼          ▼\n              Embedding    Vector DB   LLM API\n              Model        (pgvector)  (Ollama/OpenAI)</code></pre></div>\n<p>The system consists of:</p>\n<ol>\n<li><strong>Django REST API</strong> — handles authentication, rate limiting, and request routing</li>\n<li><strong>Document ingestion pipeline</strong> — processes PDFs/text, chunks them, and stores embeddings</li>\n<li><strong>Retrieval engine</strong> — performs semantic search using pgvector</li>\n<li><strong>Generation layer</strong> — Langchain chains that combine retrieved context with LLM prompting</li>\n</ol>\n<h2>Setting Up the Stack</h2>\n<h3>Django Models</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># models.py</span>\n<span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>db <span class=\"token keyword\">import</span> models\n<span class=\"token keyword\">from</span> pgvector<span class=\"token punctuation\">.</span>django <span class=\"token keyword\">import</span> VectorField\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Document</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    title <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>CharField<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">500</span><span class=\"token punctuation\">)</span>\n    source <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>FileField<span class=\"token punctuation\">(</span>upload_to<span class=\"token operator\">=</span><span class=\"token string\">'documents/'</span><span class=\"token punctuation\">)</span>\n    uploaded_at <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>DateTimeField<span class=\"token punctuation\">(</span>auto_now_add<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DocumentChunk</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    document <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>ForeignKey<span class=\"token punctuation\">(</span>Document<span class=\"token punctuation\">,</span> on_delete<span class=\"token operator\">=</span>models<span class=\"token punctuation\">.</span>CASCADE<span class=\"token punctuation\">)</span>\n    content <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>TextField<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    embedding <span class=\"token operator\">=</span> VectorField<span class=\"token punctuation\">(</span>dimensions<span class=\"token operator\">=</span><span class=\"token number\">1536</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># OpenAI ada-002</span>\n    metadata <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>JSONField<span class=\"token punctuation\">(</span>default<span class=\"token operator\">=</span><span class=\"token builtin\">dict</span><span class=\"token punctuation\">)</span>\n    chunk_index <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>IntegerField<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">class</span> <span class=\"token class-name\">Meta</span><span class=\"token punctuation\">:</span>\n        indexes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>\n            models<span class=\"token punctuation\">.</span>Index<span class=\"token punctuation\">(</span>fields<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'document'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">]</span></code></pre></div>\n<h3>Document Ingestion</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/ingestion.py</span>\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>text_splitter <span class=\"token keyword\">import</span> RecursiveCharacterTextSplitter\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>embeddings <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">ingest_document</span><span class=\"token punctuation\">(</span>document_id<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    document <span class=\"token operator\">=</span> Document<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span>document_id<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Load and split</span>\n    text <span class=\"token operator\">=</span> extract_text<span class=\"token punctuation\">(</span>document<span class=\"token punctuation\">.</span>source<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">)</span>\n    splitter <span class=\"token operator\">=</span> RecursiveCharacterTextSplitter<span class=\"token punctuation\">(</span>\n        chunk_size<span class=\"token operator\">=</span><span class=\"token number\">1000</span><span class=\"token punctuation\">,</span>\n        chunk_overlap<span class=\"token operator\">=</span><span class=\"token number\">200</span><span class=\"token punctuation\">,</span>\n        separators<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"\\n\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"\\n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\". \"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\" \"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n    chunks <span class=\"token operator\">=</span> splitter<span class=\"token punctuation\">.</span>split_text<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Embed</span>\n    embeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    vectors <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">.</span>embed_documents<span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Store</span>\n    DocumentChunk<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>bulk_create<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n        DocumentChunk<span class=\"token punctuation\">(</span>\n            document<span class=\"token operator\">=</span>document<span class=\"token punctuation\">,</span>\n            content<span class=\"token operator\">=</span>chunk<span class=\"token punctuation\">,</span>\n            embedding<span class=\"token operator\">=</span>vector<span class=\"token punctuation\">,</span>\n            chunk_index<span class=\"token operator\">=</span>i\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>chunk<span class=\"token punctuation\">,</span> vector<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">,</span> vectors<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Retrieval with pgvector</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/retrieval.py</span>\n<span class=\"token keyword\">from</span> pgvector<span class=\"token punctuation\">.</span>django <span class=\"token keyword\">import</span> L2Distance\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>embeddings <span class=\"token keyword\">import</span> OpenAIEmbeddings\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">retrieve_context</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> top_k<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span> <span class=\"token operator\">=</span> <span class=\"token number\">5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    embeddings <span class=\"token operator\">=</span> OpenAIEmbeddings<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    query_vector <span class=\"token operator\">=</span> embeddings<span class=\"token punctuation\">.</span>embed_query<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n\n    chunks <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>\n        DocumentChunk<span class=\"token punctuation\">.</span>objects\n        <span class=\"token punctuation\">.</span>annotate<span class=\"token punctuation\">(</span>distance<span class=\"token operator\">=</span>L2Distance<span class=\"token punctuation\">(</span><span class=\"token string\">'embedding'</span><span class=\"token punctuation\">,</span> query_vector<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">.</span>order_by<span class=\"token punctuation\">(</span><span class=\"token string\">'distance'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>top_k<span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> chunk<span class=\"token punctuation\">.</span>content<span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">:</span> chunk<span class=\"token punctuation\">.</span>distance<span class=\"token punctuation\">}</span>\n        <span class=\"token keyword\">for</span> chunk <span class=\"token keyword\">in</span> chunks\n    <span class=\"token punctuation\">]</span></code></pre></div>\n<h3>The RAG Chain</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/rag.py</span>\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>chains <span class=\"token keyword\">import</span> RetrievalQA\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>prompts <span class=\"token keyword\">import</span> PromptTemplate\n<span class=\"token keyword\">from</span> langchain<span class=\"token punctuation\">.</span>llms <span class=\"token keyword\">import</span> Ollama\n\nPROMPT_TEMPLATE <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"Use the following context to answer the question.\nIf you don't know the answer, say so — don't make things up.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">generate_answer</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    context_docs <span class=\"token operator\">=</span> retrieve_context<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    context <span class=\"token operator\">=</span> <span class=\"token string\">\"\\n\\n\"</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>doc<span class=\"token punctuation\">[</span><span class=\"token string\">\"content\"</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> doc <span class=\"token keyword\">in</span> context_docs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    llm <span class=\"token operator\">=</span> Ollama<span class=\"token punctuation\">(</span>model<span class=\"token operator\">=</span><span class=\"token string\">\"llama3\"</span><span class=\"token punctuation\">)</span>\n    prompt <span class=\"token operator\">=</span> PromptTemplate<span class=\"token punctuation\">(</span>\n        template<span class=\"token operator\">=</span>PROMPT_TEMPLATE<span class=\"token punctuation\">,</span>\n        input_variables<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">\"context\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"question\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n\n    chain <span class=\"token operator\">=</span> prompt <span class=\"token operator\">|</span> llm\n    response <span class=\"token operator\">=</span> chain<span class=\"token punctuation\">.</span>invoke<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"context\"</span><span class=\"token punctuation\">:</span> context<span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"question\"</span><span class=\"token punctuation\">:</span> query\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token string\">\"answer\"</span><span class=\"token punctuation\">:</span> response<span class=\"token punctuation\">,</span>\n        <span class=\"token string\">\"sources\"</span><span class=\"token punctuation\">:</span> context_docs\n    <span class=\"token punctuation\">}</span></code></pre></div>\n<h2>Django API Endpoint</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># views.py</span>\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>views <span class=\"token keyword\">import</span> APIView\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>response <span class=\"token keyword\">import</span> Response\n<span class=\"token keyword\">from</span> rest_framework<span class=\"token punctuation\">.</span>permissions <span class=\"token keyword\">import</span> IsAuthenticated\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">RAGQueryView</span><span class=\"token punctuation\">(</span>APIView<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    permission_classes <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>IsAuthenticated<span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> request<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        query <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token string\">\"query\"</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> query<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"error\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Query is required\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> status<span class=\"token operator\">=</span><span class=\"token number\">400</span><span class=\"token punctuation\">)</span>\n\n        result <span class=\"token operator\">=</span> generate_answer<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Production Considerations</h2>\n<h3>1. Chunking Strategy Matters</h3>\n<p>The quality of your RAG system depends heavily on how you split documents. We found that:</p>\n<ul>\n<li><strong>1000 tokens per chunk</strong> with <strong>200 token overlap</strong> works well for most documents</li>\n<li>Using semantic boundaries (paragraphs, sections) preserves context better than fixed-size splits</li>\n<li>Metadata-enriched chunks (source, page number, section title) improve retrieval quality</li>\n</ul>\n<h3>2. Hybrid Search</h3>\n<p>Pure vector similarity search can miss keyword-specific matches. We combine:</p>\n<ul>\n<li><strong>Semantic search</strong> (pgvector) for conceptual similarity</li>\n<li><strong>Full-text search</strong> (PostgreSQL <code class=\"language-text\">tsvector</code>) for keyword matching</li>\n<li><strong>Reciprocal Rank Fusion</strong> to combine results</li>\n</ul>\n<h3>3. Caching</h3>\n<p>For repeated queries, we cache embeddings and responses using Redis with a TTL:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> hashlib\n<span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>core<span class=\"token punctuation\">.</span>cache <span class=\"token keyword\">import</span> cache\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">cached_generate</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    cache_key <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"rag:</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>hashlib<span class=\"token punctuation\">.</span>md5<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>hexdigest<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span>\n    cached <span class=\"token operator\">=</span> cache<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>cache_key<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> cached<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> cached\n\n    result <span class=\"token operator\">=</span> generate_answer<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    cache<span class=\"token punctuation\">.</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>cache_key<span class=\"token punctuation\">,</span> result<span class=\"token punctuation\">,</span> timeout<span class=\"token operator\">=</span><span class=\"token number\">3600</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> result</code></pre></div>\n<h3>4. Evaluation</h3>\n<p>We use RAGAS metrics to evaluate retrieval quality: faithfulness, answer relevancy, and context precision. This helps us tune chunk size, overlap, and retrieval parameters.</p>\n<h2>Conclusion</h2>\n<p>Django + Langchain is a powerful combination for building production RAG systems. Django handles the \"boring but essential\" parts (auth, admin, ORM, migrations) while Langchain provides the LLM orchestration layer. With pgvector, you get a vector database without adding another service to your stack.</p>\n<hr>\n<p><em>This post is based on my experience building AI systems at Orbit Tech Solution and Bagus Harapan Tritunggal.</em></p>"}},{"node":{"frontmatter":{"title":"Physics-Informed Neural Networks: Bridging Physics and Deep Learning","description":"A deep dive into PINNs — how we combined physics engines with neural networks to predict ball trajectory in a hyper-realistic golf simulator.","slug":"/pensieve/physics-informed-neural-networks","date":"2024-11-15","tags":["Machine Learning","Physics","PyTorch","PINNs"],"draft":false},"html":"<h2>Introduction</h2>\n<p>Physics-Informed Neural Networks (PINNs) represent a paradigm shift in how we approach problems that lie at the intersection of physics and machine learning. Rather than treating neural networks as black boxes that learn purely from data, PINNs encode the fundamental laws of physics directly into the loss function — ensuring that predictions remain physically consistent.</p>\n<p>In this post, I'll share my experience building the core engine for <strong>Fairway Golf Simulator</strong>, where we used PINNs combined with traditional ANNs to predict golf ball trajectory with remarkable accuracy.</p>\n<h2>The Problem</h2>\n<p>Golf ball physics is deceptively complex. A ball in flight is affected by:</p>\n<ul>\n<li><strong>Gravity</strong> — the constant downward acceleration</li>\n<li><strong>Aerodynamic drag</strong> — resistance proportional to velocity squared</li>\n<li><strong>Magnus effect</strong> — spin-induced lift that causes hooks and slices</li>\n<li><strong>Wind</strong> — external force vectors varying by altitude</li>\n<li><strong>Ground interaction</strong> — bounce, roll, and friction on landing</li>\n</ul>\n<p>Traditional physics engines can model these forces, but they require precise initial conditions and material properties that are difficult to measure in practice. Pure data-driven approaches, on the other hand, require enormous datasets and often produce physically implausible predictions.</p>\n<h2>Why PINNs?</h2>\n<p>PINNs offer the best of both worlds. The key insight is embedding the governing differential equations directly into the neural network's training process.</p>\n<p>For a golf ball in flight, the equations of motion are:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">m * d²x/dt² = F_drag_x + F_magnus_x + F_wind_x\nm * d²y/dt² = F_drag_y + F_magnus_y + F_wind_y - m*g\nm * d²z/dt² = F_drag_z + F_magnus_z + F_wind_z</code></pre></div>\n<p>In a PINN framework, we define a composite loss function:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">loss <span class=\"token operator\">=</span> loss_data <span class=\"token operator\">+</span> lambda_physics <span class=\"token operator\">*</span> loss_physics\n\n<span class=\"token comment\"># loss_data: MSE between predictions and observed trajectories</span>\n<span class=\"token comment\"># loss_physics: Residual of the governing PDEs evaluated at collocation points</span></code></pre></div>\n<p>The physics loss acts as a regularizer, ensuring the network's predictions satisfy physical laws even in regions with sparse training data.</p>\n<h2>Architecture</h2>\n<p>Our implementation used PyTorch with the following architecture:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">GolfPINN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>net <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\"># Input: time t</span>\n            nn<span class=\"token punctuation\">.</span>Tanh<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> <span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Tanh<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> <span class=\"token number\">128</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Tanh<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\"># Output: x, y, z, vx, vy, vz</span>\n        <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>net<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span></code></pre></div>\n<p>The physics residual is computed using automatic differentiation:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">physics_loss</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    t<span class=\"token punctuation\">.</span>requires_grad_<span class=\"token punctuation\">(</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>t<span class=\"token punctuation\">)</span>\n    x<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> z <span class=\"token operator\">=</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">:</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span>\n    vx<span class=\"token punctuation\">,</span> vy<span class=\"token punctuation\">,</span> vz <span class=\"token operator\">=</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">:</span><span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">:</span><span class=\"token number\">5</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> output<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">:</span><span class=\"token number\">6</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\"># Compute accelerations via autograd</span>\n    ax <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>autograd<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>vx<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>ones_like<span class=\"token punctuation\">(</span>vx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> create_graph<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    ay <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>autograd<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>vy<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>ones_like<span class=\"token punctuation\">(</span>vy<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> create_graph<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    az <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>autograd<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">(</span>vz<span class=\"token punctuation\">,</span> t<span class=\"token punctuation\">,</span> torch<span class=\"token punctuation\">.</span>ones_like<span class=\"token punctuation\">(</span>vz<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> create_graph<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n    <span class=\"token comment\"># Physics: F = ma</span>\n    drag <span class=\"token operator\">=</span> compute_drag<span class=\"token punctuation\">(</span>vx<span class=\"token punctuation\">,</span> vy<span class=\"token punctuation\">,</span> vz<span class=\"token punctuation\">)</span>\n    magnus <span class=\"token operator\">=</span> compute_magnus<span class=\"token punctuation\">(</span>vx<span class=\"token punctuation\">,</span> vy<span class=\"token punctuation\">,</span> vz<span class=\"token punctuation\">,</span> spin<span class=\"token punctuation\">)</span>\n\n    residual_x <span class=\"token operator\">=</span> ax <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>drag<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> magnus<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> mass\n    residual_y <span class=\"token operator\">=</span> ay <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>drag<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> magnus<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">-</span> g<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> mass\n    residual_z <span class=\"token operator\">=</span> az <span class=\"token operator\">-</span> <span class=\"token punctuation\">(</span>drag<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> magnus<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> mass\n\n    <span class=\"token keyword\">return</span> torch<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>residual_x<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> residual_y<span class=\"token operator\">**</span><span class=\"token number\">2</span> <span class=\"token operator\">+</span> residual_z<span class=\"token operator\">**</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Results</h2>\n<p>The PINN model achieved significantly better generalization compared to a pure data-driven ANN, especially for:</p>\n<ul>\n<li><strong>Extreme spin conditions</strong> — where training data was sparse</li>\n<li><strong>Variable wind scenarios</strong> — the physics constraint ensured realistic deflection</li>\n<li><strong>Long-range predictions</strong> — error accumulation was much lower than pure ANNs</li>\n</ul>\n<p>The combined system now powers the Fairway Simulator, processing player input in real-time to predict trajectory with high fidelity.</p>\n<h2>Key Takeaways</h2>\n<ol>\n<li><strong>PINNs shine when data is limited</strong> — physics constraints act as powerful regularizers</li>\n<li><strong>Automatic differentiation is the secret weapon</strong> — PyTorch's autograd makes computing PDE residuals trivial</li>\n<li><strong>The physics loss weight (λ) matters</strong> — too low and you lose physical consistency, too high and you underfit the data</li>\n<li><strong>Hybrid approaches work best</strong> — we used PINNs for trajectory prediction and traditional ANNs for player skill assessment</li>\n</ol>\n<h2>What's Next</h2>\n<p>We're exploring adaptive collocation point sampling and multi-fidelity PINNs to further improve accuracy while reducing training time. The framework generalizes well beyond golf — any domain where physics models exist but data is scarce can benefit from this approach.</p>\n<hr>\n<p><em>This post is based on my work at Orbit Tech Solution building the Fairway Golf Engine.</em></p>"}},{"node":{"frontmatter":{"title":"Building a Real-World Asset Tokenization Platform with Django and Web3","description":"Lessons learned from architecting Karpous — a hybrid centralized/decentralized platform for tokenizing real-world assets using Django and Web3.","slug":"/pensieve/rwa-tokenization-django-web3","date":"2024-06-10","tags":["Django","Web3","Blockchain","Crypto"],"draft":false},"html":"<h2>Introduction</h2>\n<p>Real-World Asset (RWA) tokenization is one of the most compelling use cases for blockchain technology — representing physical assets like real estate, commodities, or intellectual property as digital tokens on a blockchain. This unlocks fractional ownership, 24/7 trading, and programmable compliance.</p>\n<p>I led the backend architecture for <strong>Karpous</strong>, a platform that enables users to invest in tokenized real-world assets. The biggest technical challenge? Building a hybrid system that seamlessly bridges traditional web infrastructure (Django, PostgreSQL) with decentralized blockchain protocols (Web3, MetaMask, smart contracts).</p>\n<h2>Architecture: Centralized Meets Decentralized</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">┌────────────────────────────────────────────────┐\n│                  Frontend (React)                │\n├────────────────────────────────────────────────┤\n│              Django REST Backend                 │\n│  ┌──────────┐  ┌──────────┐  ┌──────────────┐ │\n│  │ Auth &amp;   │  │ Asset    │  │ Transaction  │ │\n│  │ KYC      │  │ Manager  │  │ Engine       │ │\n│  └──────────┘  └──────────┘  └──────────────┘ │\n├────────────────────────────────────────────────┤\n│              Blockchain Layer                    │\n│  ┌──────────┐  ┌──────────┐  ┌──────────────┐ │\n│  │ Web3.py  │  │ Smart    │  │ MetaMask     │ │\n│  │ Gateway  │  │ Contracts│  │ Integration  │ │\n│  └──────────┘  └──────────┘  └──────────────┘ │\n├────────────────────────────────────────────────┤\n│  PostgreSQL  │  Redis  │  gRPC Services        │\n└────────────────────────────────────────────────┘</code></pre></div>\n<h2>The Hybrid Challenge</h2>\n<p>The fundamental tension in RWA platforms is: blockchain transactions are immutable and asynchronous, while your Django app expects synchronous, reversible database operations.</p>\n<h3>Transaction States</h3>\n<p>We needed a state machine to track the lifecycle of every tokenization event:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">TokenTransaction</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>Model<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">class</span> <span class=\"token class-name\">Status</span><span class=\"token punctuation\">(</span>models<span class=\"token punctuation\">.</span>TextChoices<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        PENDING <span class=\"token operator\">=</span> <span class=\"token string\">'pending'</span>          <span class=\"token comment\"># Created in Django</span>\n        SUBMITTED <span class=\"token operator\">=</span> <span class=\"token string\">'submitted'</span>      <span class=\"token comment\"># Sent to blockchain</span>\n        CONFIRMING <span class=\"token operator\">=</span> <span class=\"token string\">'confirming'</span>    <span class=\"token comment\"># Waiting for confirmations</span>\n        CONFIRMED <span class=\"token operator\">=</span> <span class=\"token string\">'confirmed'</span>      <span class=\"token comment\"># On-chain confirmed</span>\n        FAILED <span class=\"token operator\">=</span> <span class=\"token string\">'failed'</span>            <span class=\"token comment\"># Transaction reverted</span>\n\n    asset <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>ForeignKey<span class=\"token punctuation\">(</span><span class=\"token string\">'Asset'</span><span class=\"token punctuation\">,</span> on_delete<span class=\"token operator\">=</span>models<span class=\"token punctuation\">.</span>PROTECT<span class=\"token punctuation\">)</span>\n    user <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>ForeignKey<span class=\"token punctuation\">(</span><span class=\"token string\">'User'</span><span class=\"token punctuation\">,</span> on_delete<span class=\"token operator\">=</span>models<span class=\"token punctuation\">.</span>PROTECT<span class=\"token punctuation\">)</span>\n    tx_hash <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>CharField<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">66</span><span class=\"token punctuation\">,</span> null<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> blank<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    status <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>CharField<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> choices<span class=\"token operator\">=</span>Status<span class=\"token punctuation\">.</span>choices<span class=\"token punctuation\">)</span>\n    amount <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>DecimalField<span class=\"token punctuation\">(</span>max_digits<span class=\"token operator\">=</span><span class=\"token number\">24</span><span class=\"token punctuation\">,</span> decimal_places<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span>\n    created_at <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>DateTimeField<span class=\"token punctuation\">(</span>auto_now_add<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    confirmed_at <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>DateTimeField<span class=\"token punctuation\">(</span>null<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Web3 Integration</h3>\n<p>We built a gateway service to interact with smart contracts:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># services/web3_gateway.py</span>\n<span class=\"token keyword\">from</span> web3 <span class=\"token keyword\">import</span> Web3\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">Web3Gateway</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>w3 <span class=\"token operator\">=</span> Web3<span class=\"token punctuation\">(</span>Web3<span class=\"token punctuation\">.</span>HTTPProvider<span class=\"token punctuation\">(</span>settings<span class=\"token punctuation\">.</span>RPC_URL<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>contract <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>contract<span class=\"token punctuation\">(</span>\n            address<span class=\"token operator\">=</span>settings<span class=\"token punctuation\">.</span>TOKEN_CONTRACT_ADDRESS<span class=\"token punctuation\">,</span>\n            abi<span class=\"token operator\">=</span>TOKEN_ABI\n        <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">mint_tokens</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> to_address<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> amount<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">,</span> asset_id<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"Mint RWA tokens to a user's wallet.\"\"\"</span>\n        tx <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>contract<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span>mint<span class=\"token punctuation\">(</span>\n            to_address<span class=\"token punctuation\">,</span> amount<span class=\"token punctuation\">,</span> asset_id\n        <span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>build_transaction<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n            <span class=\"token string\">'from'</span><span class=\"token punctuation\">:</span> settings<span class=\"token punctuation\">.</span>ADMIN_WALLET<span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'nonce'</span><span class=\"token punctuation\">:</span> self<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>get_transaction_count<span class=\"token punctuation\">(</span>\n                settings<span class=\"token punctuation\">.</span>ADMIN_WALLET\n            <span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'gas'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">200000</span><span class=\"token punctuation\">,</span>\n            <span class=\"token string\">'gasPrice'</span><span class=\"token punctuation\">:</span> self<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>gas_price<span class=\"token punctuation\">,</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n        signed <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>account<span class=\"token punctuation\">.</span>sign_transaction<span class=\"token punctuation\">(</span>\n            tx<span class=\"token punctuation\">,</span> settings<span class=\"token punctuation\">.</span>ADMIN_PRIVATE_KEY\n        <span class=\"token punctuation\">)</span>\n        tx_hash <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>send_raw_transaction<span class=\"token punctuation\">(</span>\n            signed<span class=\"token punctuation\">.</span>raw_transaction\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> tx_hash<span class=\"token punctuation\">.</span><span class=\"token builtin\">hex</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>The Confirmation Problem</h3>\n<p>Blockchain transactions aren't instant. We used Celery workers to poll for confirmations:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># tasks.py</span>\n<span class=\"token keyword\">from</span> celery <span class=\"token keyword\">import</span> shared_task\n\n<span class=\"token decorator annotation punctuation\">@shared_task</span><span class=\"token punctuation\">(</span>bind<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> max_retries<span class=\"token operator\">=</span><span class=\"token number\">50</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">check_transaction_confirmation</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> transaction_id<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    tx <span class=\"token operator\">=</span> TokenTransaction<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span>transaction_id<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n        receipt <span class=\"token operator\">=</span> web3_gateway<span class=\"token punctuation\">.</span>w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>get_transaction_receipt<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">.</span>tx_hash<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> receipt <span class=\"token keyword\">is</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Not mined yet — retry in 15 seconds</span>\n            <span class=\"token keyword\">raise</span> self<span class=\"token punctuation\">.</span>retry<span class=\"token punctuation\">(</span>countdown<span class=\"token operator\">=</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> receipt<span class=\"token punctuation\">[</span><span class=\"token string\">'status'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">==</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span>\n            tx<span class=\"token punctuation\">.</span>status <span class=\"token operator\">=</span> TokenTransaction<span class=\"token punctuation\">.</span>Status<span class=\"token punctuation\">.</span>CONFIRMED\n            tx<span class=\"token punctuation\">.</span>confirmed_at <span class=\"token operator\">=</span> timezone<span class=\"token punctuation\">.</span>now<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            tx<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            notify_user_confirmation<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            tx<span class=\"token punctuation\">.</span>status <span class=\"token operator\">=</span> TokenTransaction<span class=\"token punctuation\">.</span>Status<span class=\"token punctuation\">.</span>FAILED\n            tx<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            handle_failed_transaction<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">except</span> TransactionNotFound<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">raise</span> self<span class=\"token punctuation\">.</span>retry<span class=\"token punctuation\">(</span>countdown<span class=\"token operator\">=</span><span class=\"token number\">15</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Security Considerations</h2>\n<h3>Wallet Verification</h3>\n<p>Users connect their MetaMask wallet and sign a message to prove ownership:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># views.py</span>\n<span class=\"token keyword\">from</span> eth_account<span class=\"token punctuation\">.</span>messages <span class=\"token keyword\">import</span> defunct_hash_message\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">WalletVerifyView</span><span class=\"token punctuation\">(</span>APIView<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">post</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> request<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        address <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">[</span><span class=\"token string\">'address'</span><span class=\"token punctuation\">]</span>\n        signature <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">[</span><span class=\"token string\">'signature'</span><span class=\"token punctuation\">]</span>\n        message <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"Verify wallet for Karpous: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>request<span class=\"token punctuation\">.</span>user<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span>\n\n        <span class=\"token comment\"># Recover signer address from signature</span>\n        message_hash <span class=\"token operator\">=</span> defunct_hash_message<span class=\"token punctuation\">(</span>text<span class=\"token operator\">=</span>message<span class=\"token punctuation\">)</span>\n        recovered <span class=\"token operator\">=</span> w3<span class=\"token punctuation\">.</span>eth<span class=\"token punctuation\">.</span>account<span class=\"token punctuation\">.</span>recover_message<span class=\"token punctuation\">(</span>\n            message_hash<span class=\"token punctuation\">,</span> signature<span class=\"token operator\">=</span>signature\n        <span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">if</span> recovered<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> address<span class=\"token punctuation\">.</span>lower<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            request<span class=\"token punctuation\">.</span>user<span class=\"token punctuation\">.</span>wallet_address <span class=\"token operator\">=</span> address\n            request<span class=\"token punctuation\">.</span>user<span class=\"token punctuation\">.</span>wallet_verified <span class=\"token operator\">=</span> <span class=\"token boolean\">True</span>\n            request<span class=\"token punctuation\">.</span>user<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"status\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"verified\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> Response<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"error\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Invalid signature\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> status<span class=\"token operator\">=</span><span class=\"token number\">400</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Double-Spend Prevention</h3>\n<p>We use database-level locking to prevent race conditions:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> django<span class=\"token punctuation\">.</span>db <span class=\"token keyword\">import</span> transaction\n\n<span class=\"token decorator annotation punctuation\">@transaction<span class=\"token punctuation\">.</span>atomic</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">purchase_tokens</span><span class=\"token punctuation\">(</span>user<span class=\"token punctuation\">,</span> asset<span class=\"token punctuation\">,</span> amount<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    asset <span class=\"token operator\">=</span> Asset<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>select_for_update<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token builtin\">id</span><span class=\"token operator\">=</span>asset<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">if</span> asset<span class=\"token punctuation\">.</span>available_supply <span class=\"token operator\">&lt;</span> amount<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">raise</span> InsufficientSupplyError<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    asset<span class=\"token punctuation\">.</span>available_supply <span class=\"token operator\">-=</span> amount\n    asset<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    tx <span class=\"token operator\">=</span> TokenTransaction<span class=\"token punctuation\">.</span>objects<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        user<span class=\"token operator\">=</span>user<span class=\"token punctuation\">,</span> asset<span class=\"token operator\">=</span>asset<span class=\"token punctuation\">,</span>\n        amount<span class=\"token operator\">=</span>amount<span class=\"token punctuation\">,</span> status<span class=\"token operator\">=</span><span class=\"token string\">'pending'</span>\n    <span class=\"token punctuation\">)</span>\n\n    tx_hash <span class=\"token operator\">=</span> web3_gateway<span class=\"token punctuation\">.</span>mint_tokens<span class=\"token punctuation\">(</span>\n        user<span class=\"token punctuation\">.</span>wallet_address<span class=\"token punctuation\">,</span> amount<span class=\"token punctuation\">,</span> asset<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span>\n    <span class=\"token punctuation\">)</span>\n    tx<span class=\"token punctuation\">.</span>tx_hash <span class=\"token operator\">=</span> tx_hash\n    tx<span class=\"token punctuation\">.</span>status <span class=\"token operator\">=</span> <span class=\"token string\">'submitted'</span>\n    tx<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    check_transaction_confirmation<span class=\"token punctuation\">.</span>delay<span class=\"token punctuation\">(</span>tx<span class=\"token punctuation\">.</span><span class=\"token builtin\">id</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> tx</code></pre></div>\n<h2>gRPC for Internal Services</h2>\n<p>For high-frequency internal communication between microservices, we used gRPC instead of REST:</p>\n<div class=\"gatsby-highlight\" data-language=\"protobuf\"><pre class=\"language-protobuf\"><code class=\"language-protobuf\"><span class=\"token keyword\">service</span> <span class=\"token class-name\">AssetService</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">rpc</span> <span class=\"token function\">GetAssetPrice</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">AssetRequest</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">returns</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">PriceResponse</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">rpc</span> <span class=\"token function\">StreamPriceUpdates</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">AssetRequest</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">returns</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">stream</span> <span class=\"token class-name\">PriceResponse</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>This gave us ~10x lower latency compared to REST for price feed updates.</p>\n<h2>Lessons Learned</h2>\n<ol>\n<li><strong>Never trust the frontend for blockchain state</strong> — always verify on-chain</li>\n<li><strong>Idempotency is critical</strong> — network failures mean you might submit a transaction twice</li>\n<li><strong>Gas estimation is tricky</strong> — always add a buffer and implement retry logic</li>\n<li><strong>Test on testnets extensively</strong> — mainnet debugging is expensive</li>\n<li><strong>Separate hot and cold wallets</strong> — the admin wallet for minting should have limited funds</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building Karpous taught me that the hardest part of Web3 development isn't the blockchain itself — it's the glue between your traditional backend and the decentralized world. Django's robust ORM, transaction management, and async task processing (Celery) make it an excellent foundation for hybrid Web3 applications.</p>\n<hr>\n<p><em>This post is based on my work at Orbit Tech Solution building the Karpous RWA platform.</em></p>"}},{"node":{"frontmatter":{"title":"Docker Compose Error","description":"docker-compose version discrepancies","slug":"/pensieve/docker-error","date":"2019-12-13","tags":["WordPress","Docker"],"draft":false},"html":"<h2>Problem</h2>\n<p>Recently while updating with <a href=\"https://github.com/Upstatement/skela-wp-theme\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Skela</a> with webpack, I encountered a weird error where I wasn't able to run a simple script:</p>\n<div class=\"gatsby-code-title\">bin/composer</div>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token shebang important\">#!/bin/bash</span>\n<span class=\"token function\">docker-compose</span> <span class=\"token builtin class-name\">exec</span> <span class=\"token parameter variable\">-w</span> /var/www/html/wp-content/themes/skela wordpress <span class=\"token function\">composer</span> <span class=\"token string\">\"<span class=\"token variable\">$@</span>\"</span></code></pre></div>\n<p>When trying to run this script via <code class=\"language-text\">./bin/composer install</code>, I got this error in my terminal:</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">ERROR: Setting workdir <span class=\"token keyword\">for</span> <span class=\"token builtin class-name\">exec</span> is not supported <span class=\"token keyword\">in</span> API <span class=\"token operator\">&lt;</span> <span class=\"token number\">1.35</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.30</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The error was coming from the <code class=\"language-text\">-w</code> flag in the <code class=\"language-text\">docker-compose exec</code> command in the <code class=\"language-text\">composer</code> script.</p>\n<h2>Solution</h2>\n<p>Turns The fix was to update the version in my <code class=\"language-text\">docker-compose.yml</code> file to from version <code class=\"language-text\">3.5</code> to <code class=\"language-text\">3.6</code>. It's strange because 3.5 isn't anywhere close to the API version <code class=\"language-text\">1.35</code> from the error message 🤷‍♀️</p>\n<div class=\"gatsby-code-title\">docker-compose.yml</div>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"gatsby-highlight-code-line\"><span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'3.6'</span></span><span class=\"token key atrule\">services</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">wordpress</span><span class=\"token punctuation\">:</span>\n    build<span class=\"token punctuation\">:</span></code></pre></div>"}},{"node":{"frontmatter":{"title":"WordPress Publishing Error","description":"Trying to create a simple post in WordPress","slug":"/pensieve/wordpress-publish-error","date":"2019-12-03","tags":["WordPress"],"draft":false},"html":"<h2>Problem</h2>\n<p>Recently while working on a WordPress project with <a href=\"https://github.com/Upstatement/ups-dock\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ups Dock</a>, I encountered a weird error where I wasn't able to update or publish a simple post in my local WP admin.</p>\n<p>It looked something like this:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9a7943feb3c4ee95b43801cfda486beb/8e621/draft-fail.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32.57142857142857%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABBElEQVR42qVQu07EMBD05/J3SDTUdBRUiKvggnK5PC/B8Tk5r5/DOhINQmkYaWSvdnc8HkFECN7DWYuUElKMG7nY5UoemhxgDR4PJe7uX/Dw9Awhj5+gukbsBwRmuozwXQ/XtLs01Rlj06FqB7S8044SUl0heharyxJyGDB1HU5FATWOgHOIxiDxD34zMnN/Vhrv5xaX6WtzHZmiYZGPY4GKhU/86uvbAXKekZEj2INj0XVZtjnyEXK1ECEEWM7PWscDnCXneWNnIef4B/I8kUXkPvGeZkHD53RdsHCmIjfykGfmu/dhq/fw4zw71FpDyRmezPZtgX8gGzCc56oUbhwTGYNvRIsfDCxjw1gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Draft fail\"\n        title=\"\"\n        src=\"/static/9a7943feb3c4ee95b43801cfda486beb/39600/draft-fail.png\"\n        srcset=\"/static/9a7943feb3c4ee95b43801cfda486beb/1aaec/draft-fail.png 175w,\n/static/9a7943feb3c4ee95b43801cfda486beb/98287/draft-fail.png 350w,\n/static/9a7943feb3c4ee95b43801cfda486beb/39600/draft-fail.png 700w,\n/static/9a7943feb3c4ee95b43801cfda486beb/57cd1/draft-fail.png 1050w,\n/static/9a7943feb3c4ee95b43801cfda486beb/4af54/draft-fail.png 1400w,\n/static/9a7943feb3c4ee95b43801cfda486beb/8e621/draft-fail.png 2234w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Sometimes the error message would be slightly more helpful: <code class=\"language-text\">Publishing failed. Error message: The response is not a valid JSON response.</code></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c2362fe43c3b6f9628b1cc63d0bb00f9/04410/publish-error.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 12%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAc0lEQVR42i2Oaw6DMAyDuf/dqChtaFqGEK/x2KYdwSQRPz4lTmzJ1a/vsVOHM7Hxyfmh4GTGtxTTl8FQv6L3i7Nl9X9I9v8aUGloahosvsXSBmyB8CbCFglD7Ww/uiQej9l707to9a4xYg0yQ8TonBRKuAFq8Y7nU3DtiQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Publish error\"\n        title=\"\"\n        src=\"/static/c2362fe43c3b6f9628b1cc63d0bb00f9/39600/publish-error.png\"\n        srcset=\"/static/c2362fe43c3b6f9628b1cc63d0bb00f9/1aaec/publish-error.png 175w,\n/static/c2362fe43c3b6f9628b1cc63d0bb00f9/98287/publish-error.png 350w,\n/static/c2362fe43c3b6f9628b1cc63d0bb00f9/39600/publish-error.png 700w,\n/static/c2362fe43c3b6f9628b1cc63d0bb00f9/04410/publish-error.png 956w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>And if I popped open the console, I saw these errors:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 700px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/0dc960ad57313c7b34df2794f8954458/fb77c/console-errors.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20.571428571428573%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA2klEQVR42k2PS4+CMBSFSVy4MoEMMUwUEHzs3LhxEsZk0JkWoYDz/3/M8StuXNyc9va8Gvx8xGo3G9kslQHrNFV/OOiWZarBa1nKFIW63U52u1W73+uXtz92rd+B0565JJ8KvhYLjSyb5VID4n9II+PxgWHvhVk+cfo8lyO0xXCEO3DuGVcWuqA/h6GCaxipoZVNEnUQzXqtB2bNaiVDqn034O7H8nbH3BfwIV3x4lRRpOA0n2uE0DENLRwCh+FAO8f3DGG+lUN4R+QQe5xC4FZxrIp23+BxNtMTpEZ48rdeozIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Console errors\"\n        title=\"\"\n        src=\"/static/0dc960ad57313c7b34df2794f8954458/39600/console-errors.png\"\n        srcset=\"/static/0dc960ad57313c7b34df2794f8954458/1aaec/console-errors.png 175w,\n/static/0dc960ad57313c7b34df2794f8954458/98287/console-errors.png 350w,\n/static/0dc960ad57313c7b34df2794f8954458/39600/console-errors.png 700w,\n/static/0dc960ad57313c7b34df2794f8954458/57cd1/console-errors.png 1050w,\n/static/0dc960ad57313c7b34df2794f8954458/fb77c/console-errors.png 1185w\"\n        sizes=\"(max-width: 700px) 100vw, 700px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h2>Solution</h2>\n<p>Since the error message had to do with a JSON response, I initially thought it was a Gutenberg or ACF issue. But it turned out this was happening because I was on the https WP admin (i.e. <a href=\"https://project.ups.dock/wp-admin\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://project.ups.dock/wp-admin</a>), not the unsecure WP admin (<a href=\"http://project.ups.dock/wp-admin\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">http://project.ups.dock/wp-admin</a>).</p>\n<p>It was a CORS error!! I was trying to modify a non-https domain from a https domain. Switching to a non-https WP admin allowed me to publish posts with no problem.</p>"}}]}},"pageContext":{}},"staticQueryHashes":["3115057458"],"slicesMap":{}}